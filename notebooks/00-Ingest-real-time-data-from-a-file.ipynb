{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f15463-d2ba-47fd-9361-828b015b71a9",
   "metadata": {},
   "source": [
    "# Simulate Real-Time Data Reading from Historical Nasdaq Trades\n",
    "\n",
    "This notebook will read the `./tradesNasdaq.csv` file to load trading events and will simulate real-time data by inserting the events directly into QuestDB using multiple processes in parallel.\n",
    "\n",
    "The CSV file contains about half a million rows of trades observed over four days in March on Nasdaq for the symbols: `AMZN`, `AAPL`, `MSFT`, `GOOG`, `NVDA`, `META`, `TSLA`, `NFLX`, `ORCL`, `QSG`, and `BABA`. To simulate real-time behavior, the script overrides the original date with the current date and waits 50ms between events before sending them to QuestDB. You can customize these settings by modifying the constants in the script.\n",
    "\n",
    "This script will continue sending data until you stop the notebook or until the `TOTAL_EVENTS` limit is reached. If the number of events in the CSV is smaller than `TOTAL_EVENTS`, the script will simply loop over the file again.\n",
    "\n",
    "The data is stored in a table named `trades`, with the following schema. If the table does not exist, it will be created automatically on the first write:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE 'trades' (\n",
    "    symbol SYMBOL CAPACITY 256 CACHE,\n",
    "    side SYMBOL CAPACITY 256 CACHE,\n",
    "    price DOUBLE,\n",
    "    amount DOUBLE,\n",
    "    timestamp TIMESTAMP\n",
    ") timestamp(timestamp) PARTITION BY DAY WAL;\n",
    "```\n",
    "\n",
    "To view live data in your database, open a new browser tab and navigate to `http://localhost:9000`. You can execute a simple query like:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM trades -10;\n",
    "```\n",
    "\n",
    "to see the latest 10 trades. Or try a slightly more advanced query like:\n",
    "\n",
    "```sql\n",
    "SELECT timestamp, symbol, side, sum(price * amount)\n",
    "FROM trades\n",
    "SAMPLE BY 1m;\n",
    "```\n",
    "\n",
    "to see trade totals per symbol at 1-minute intervals.\n",
    "\n",
    "For more realistic queries, open the **Examples-of-market-data-queries** notebook in a new tab. It includes queries adapted from the demo machine that should return useful results for your dataset.\n",
    "\n",
    "To visualize your live data in real time, we offer a sample dashboard powered by Grafana and another one powered by Pulse. \n",
    "\n",
    "To see the Grafana dashboard navigate to the [demo Grafana dashboard](http://localhost:3000/d/live-trades-demo/live-trades-demo). The user is `admin` and the password is `quest`.\n",
    "\n",
    "To see your live data on a Pulse dashboard, please navigate in a new tab to [the demo Pulse dashboard](http://localhost:8080/dash/29/Live%20Trades%20Demo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fa8cc-82fb-4ca2-928d-166c4ca9b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion started. Connecting to host.docker.internal:9000\n",
      "Sender 0 will send 500000 events\n",
      "Sender 1 will send 500000 events\n",
      "Sender 1 started sending events\n",
      "Sender 0 started sending events\n"
     ]
    }
   ],
   "source": [
    "from questdb.ingress import Sender, IngressError, TimestampNanos\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "HTTP_ENDPOINT = os.getenv('QUESTDB_HTTP_ENDPOINT', 'questdb:9000')\n",
    "REST_TOKEN = os.getenv('QUESTDB_REST_TOKEN')\n",
    "\n",
    "TOTAL_EVENTS = 1000000  # Total events across all senders\n",
    "DELAY_MS = 50  # Delay between events in milliseconds\n",
    "NUM_SENDERS = 2  # Number of senders to execute in parallel\n",
    "CSV_FILE = './tradesNasdaq.csv'  # Path to the CSV file\n",
    "TIMESTAMP_FROM_FILE = False  # Whether to use the timestamp from the CSV file\n",
    "\n",
    "def send(sender_id, total_events, delay_ms=DELAY_MS, csv_file=CSV_FILE, http_endpoint=HTTP_ENDPOINT, auth=REST_TOKEN):\n",
    "    sys.stdout.write(f\"Sender {sender_id} will send {total_events} events\\n\")\n",
    "\n",
    "    try:\n",
    "        if auth is not None:\n",
    "            conf = f'https::addr={http_endpoint};tls_verify=unsafe_off;token={auth};'\n",
    "        else:\n",
    "            conf = f'http::addr={http_endpoint};'\n",
    "            \n",
    "        with Sender.from_conf(conf) as sender, open(csv_file, mode='r') as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "            events_sent = 0\n",
    "            csv_rows = list(csv_reader)  # Load the CSV data once into memory for looping\n",
    "            sys.stdout.write(f\"Sender {sender_id} started sending events\\n\")\n",
    "            while events_sent < total_events:\n",
    "                row = csv_rows[events_sent % len(csv_rows)]  # Loop over the CSV rows\n",
    "\n",
    "                if TIMESTAMP_FROM_FILE:\n",
    "                    timestamp_dt = datetime.strptime(row['timestamp'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    timestamp_nanos = TimestampNanos(int(timestamp_dt.timestamp() * 1e9))  # Convert to nanoseconds\n",
    "                else:\n",
    "                    timestamp_nanos = TimestampNanos.now()  # Get current time in nanoseconds\n",
    "                \n",
    "                # Ingest the row with the current timestamp\n",
    "                sender.row(\n",
    "                    'trades',\n",
    "                    symbols={'symbol': row['symbol'], 'side': row['side']},\n",
    "                    columns={\n",
    "                        'price': float(row['price']),\n",
    "                        'amount': float(row['amount']),\n",
    "                    },\n",
    "                    at=timestamp_nanos  # Send timestamp in nanoseconds\n",
    "                )\n",
    "\n",
    "                events_sent += 1\n",
    "\n",
    "                # Delay after each event\n",
    "                if delay_ms > 0:\n",
    "                    time.sleep(delay_ms / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "            sys.stdout.write(f\"Sender {sender_id} finished sending {events_sent} events\\n\")\n",
    "\n",
    "    except IngressError as e:\n",
    "        sys.stderr.write(f'Sender {sender_id} got error: {e}\\n')\n",
    "\n",
    "def parallel_send(total_events, num_senders: int):\n",
    "    events_per_sender = total_events // num_senders\n",
    "    remaining_events = total_events % num_senders\n",
    "\n",
    "    sender_events = [events_per_sender] * num_senders\n",
    "    for i in range(remaining_events):  # Distribute the remaining events\n",
    "        sender_events[i] += 1\n",
    "\n",
    "    with Pool(processes=num_senders) as pool:\n",
    "        sender_ids = range(num_senders)\n",
    "        pool.starmap(send, [(sender_id, sender_events[sender_id]) for sender_id in sender_ids])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stdout.write(f'Ingestion started. Connecting to {HTTP_ENDPOINT}\\n')\n",
    "    parallel_send(TOTAL_EVENTS, NUM_SENDERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
