{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of ingesting US Equities data \n",
    "\n",
    "This example captures US Equities data using [DatabentoÂ´s API](https://databento.com/docs/venues-and-datasets/equs-mini) and stores the data into QuestDB.\n",
    "\n",
    "In order to execute this script, you need to provide a Databento API. If you don't have one, you can register and start a free trial. Please refer to [Databento pricing](https://databento.com/pricing) for more details.\n",
    "\n",
    "The script will connect to the Databento API and subscribe to live data about US Equities trades for the symbols `AMZN,AAPL,MSFT,GOOG,NVDA,META,TSLA,NFLX,ORCL,QSG,BABA`. Depending on the time you are executing the script, it might be the case there is very low trading volume, in which case you can just comment the line with the symbols on the script to capture the data from the feed for all available symbols.\n",
    "\n",
    "The way Databento API provides the data, it sends messages of different type within the same feed. For the purposes of this demo, a message will be either a Trade (TradeMsg) or a Symbol Mapping (SymbolMappingMsg). Each trading message comes with an `instrument_id` and we can use the mapping messages to look up the symbol name. The script keeps an `instrument` array in memory mapping `id` to `name`. \n",
    "\n",
    "In this case, we are using the Python QuestDB client library for convenience, but QuestDB offers similar native clients for C/C++, Java, .Net, Rust, Go, and JavaScript.\n",
    "\n",
    "Each trading message follows [this schema](https://databento.com/docs/schemas-and-data-formats/trades?historical=python&live=python&reference=python). The script below processes each message and it stores it\n",
    "on QuestDB, in a table named `trades`, with the schema below. If table does not exist, it will be automatically created on the first write.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE 'trades' ( \n",
    "\tsymbol SYMBOL CAPACITY 256 CACHE,\n",
    "\tside SYMBOL CAPACITY 256 CACHE,\n",
    "\tprice DOUBLE,\n",
    "\tamount DOUBLE,\n",
    "\ttimestamp TIMESTAMP\n",
    ") timestamp(timestamp) PARTITION BY DAY WAL;\n",
    "```\n",
    "\n",
    "To see the live data on your database, you can open a new tab on your browser and navigate to `http://localhost:9000`. You can then execute a simple query like `SELECT * FROM trades -10;` to see the latest 10 trades. Or you could execute a sligthly more sophisticated query like `select timestamp, symbol, side, sum(price * amount) from trades sample by 1m ` to get the totals for each symbol at 1 minute intervals.\n",
    "\n",
    "If you want some more realistic queries, please open in a new tab the [Examples-of-market-data-queries notebook](/notebooks/Examples-of-market-data-queries.ipynb), where you will find some queries adapted from the demo machine that should return results for your dataset.\n",
    "\n",
    "If you want to see your live data on a real-time dashboard, please navigate in a new tab to [the demo dashboard](http://localhost:3000/d/trades-crypto-currency/trades-crypto-currency?orgId=1&refresh=250ms) powered by Grafana. The user is `admin` and password `quest`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process will run in the background until the notebook is stopped.\n"
     ]
    }
   ],
   "source": [
    "import databento as db\n",
    "from datetime import datetime\n",
    "from questdb.ingress import Sender, TimestampNanos\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DATABENTO_API_KEY = os.getenv('DATABENTO_API_KEY', '<YOUR_API_KEY>') \n",
    "\n",
    "# Opening the connection to the QuestDB instance \n",
    "questdb_conf = \"http::addr=host.docker.internal:9000;auto_flush=on;auto_flush_rows=100;auto_flush_interval=1000;\"\n",
    "sender = Sender.from_conf(questdb_conf)\n",
    "sender.establish()\n",
    "# First we build up the static data dictionary\n",
    "instruments = {}\n",
    "\n",
    "# Enable some basic logging\n",
    "db.enable_logging(\"WARN\")\n",
    "\n",
    "# Create a live client and connect\n",
    "live_client = db.Live(\n",
    "    key=DATABENTO_API_KEY,\n",
    "    reconnect_policy=\"reconnect\"\n",
    ")\n",
    "\n",
    "# Subscribe to the ohlcv-1s schema for AMZN\n",
    "live_client.subscribe(\n",
    "    dataset=\"EQUS.MINI\",\n",
    "    schema=\"trades\",\n",
    "    stype_in=\"raw_symbol\",\n",
    "    #you can comment the next line if you want to get all the stocks\n",
    "    #this is specially interesting if executing this outside of Nasdaq market hours\n",
    "    symbols=\"AMZN,AAPL,MSFT,GOOG,NVDA,META,TSLA,NFLX,ORCL,QSG,BABA\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create a callback to handle DBN records\n",
    "def record_callback(record: db.DBNRecord) -> None:    \n",
    "  if isinstance(record, db.SymbolMappingMsg):     \n",
    "    instruments.update({record.hd.instrument_id : record.stype_out_symbol})\n",
    "      \n",
    "  if isinstance(record, db.TradeMsg):  \n",
    "    instrument = instruments[record.instrument_id]\n",
    "    size = record.size\n",
    "    if record.action == 'A':\n",
    "        side='ask'\n",
    "    else:\n",
    "        side='buy'\n",
    "        \n",
    "    # databento provides the price where every 1 unit corresponds to 1e-9, i.e. 1/1,000,000,000 or 0.000000001.    \n",
    "    price = record.price * 0.000000001   \n",
    "\n",
    "    # This is where we send data to the QuestDB instance  \n",
    "    sender.row(\n",
    "      'trades',\n",
    "    symbols={'symbol': instrument},\n",
    "    columns={'amount': size,\n",
    "             'price': price,\n",
    "             'side': side\n",
    "            },\n",
    "             at=TimestampNanos(record.ts_event)\n",
    "    )\n",
    "\n",
    "   \n",
    "\n",
    "# Create a callback to handle reconnections\n",
    "def reconnect_callback(start, end) -> None:\n",
    "    sender.flush()\n",
    "    sender.close()\n",
    "    sender.establish()\n",
    "    print(f\"reconnection gap from {start} to {end}\")\n",
    "\n",
    "\n",
    "# Create a callback to handle exceptions from `user_callback`\n",
    "def error_handler(exception: Exception) -> None:\n",
    "    print(f\"an error occurred {exception}\")\n",
    "\n",
    "# The callback we register here will be invoked for each record\n",
    "live_client.add_callback(\n",
    "    record_callback=record_callback,\n",
    "    exception_callback=error_handler\n",
    ")\n",
    "\n",
    "live_client.add_reconnect_callback(\n",
    "    reconnect_callback=reconnect_callback,\n",
    "    exception_callback=error_handler,  # optional error handler\n",
    ")\n",
    "\n",
    "# Start the live client to begin data streaming\n",
    "live_client.start()\n",
    "\n",
    "print(\"The process will run in the background until the notebook is stopped.\")\n",
    "\n",
    "# Run the stream for 15 seconds before closing\n",
    "live_client.block_for_close(timeout=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
