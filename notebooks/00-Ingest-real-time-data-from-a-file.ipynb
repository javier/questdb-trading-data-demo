{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f15463-d2ba-47fd-9361-828b015b71a9",
   "metadata": {},
   "source": [
    "# Simulate real-time data reading from historical Nasdaq trades on a file\n",
    "\n",
    "This notebook will read the `./tradesNasdaq.csv` file to read trading events, and will simulate real-time data by inserting the events directly to QuestDB using multiple process in parallel. \n",
    "\n",
    "The CSV file contains about half a million rows of trades observed over four days of Nasdaq in March for the symbols `AMZN,AAPL,MSFT,GOOG,NVDA,META,TSLA,NFLX,ORCL,QSG,BABA`. To simulate real-time behaviour, the script will override the original date with the current date and will wait 50ms between events before sending to QuestDB. You can override those configurations by changing the constants in the script.\n",
    "\n",
    "This script will keep sending data until you click stop or exit the notebook, or until the TOTAL_EVENTS number is reached. If the number of events on the CSV is smaller than the total events configured, the script will sumply loop over the file again.\n",
    "\n",
    "The data is stored in a table named `trades`, with the schema below. If table does not exist, it will be automatically created on the first write.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE 'trades' ( \n",
    "    symbol SYMBOL CAPACITY 256 CACHE,\n",
    "    side SYMBOL CAPACITY 256 CACHE,\n",
    "    price DOUBLE,\n",
    "    amount DOUBLE,\n",
    "    timestamp TIMESTAMP\n",
    ") timestamp(timestamp) PARTITION BY DAY WAL;\n",
    "```\n",
    "\n",
    "To see the live data on your database, you can open a new tab on your browser and navigate to `http://localhost:9000`. You can then execute a simple query like `SELECT * FROM trades -10;` to see the latest 10 trades. Or you could execute a sligthly more sophisticated query like `select timestamp, symbol, side, sum(price * amount) from trades sample by 1m;`  to get the totals for each symbol at 1 minute intervals.\n",
    "\n",
    "For more realistic queries, please open in a new tab the [Examples-of-market-data-queries notebook](/notebooks/Examples-of-market-data-queries.ipynb), where you will find some queries adapted from the demo machine that should return results for your dataset.\n",
    "\n",
    "If you want to see your live data on a real-time dashboard, please navigate in a new tab to [the demo dashboard](http://localhost:3000/d/live-trades-demo/live-trades-demo) powered by Grafana. The user is `admin` and password `quest`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fa8cc-82fb-4ca2-928d-166c4ca9b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion started. Connecting to host.docker.internal:9000\n",
      "Sender 0 will send 500000 events\n",
      "Sender 1 will send 500000 events\n",
      "Sender 1 started sending events\n",
      "Sender 0 started sending events\n"
     ]
    }
   ],
   "source": [
    "from questdb.ingress import Sender, IngressError, TimestampNanos\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "HTTP_ENDPOINT = os.getenv('QUESTDB_HTTP_ENDPOINT', 'questdb:9000')\n",
    "REST_TOKEN = os.getenv('QUESTDB_REST_TOKEN')\n",
    "\n",
    "TOTAL_EVENTS = 1000000  # Total events across all senders\n",
    "DELAY_MS = 50  # Delay between events in milliseconds\n",
    "NUM_SENDERS = 2  # Number of senders to execute in parallel\n",
    "CSV_FILE = './tradesNasdaq.csv'  # Path to the CSV file\n",
    "TIMESTAMP_FROM_FILE = False  # Whether to use the timestamp from the CSV file\n",
    "\n",
    "def send(sender_id, total_events, delay_ms=DELAY_MS, csv_file=CSV_FILE, http_endpoint=HTTP_ENDPOINT, auth=REST_TOKEN):\n",
    "    sys.stdout.write(f\"Sender {sender_id} will send {total_events} events\\n\")\n",
    "\n",
    "    try:\n",
    "        if auth is not None:\n",
    "            conf = f'https::addr={http_endpoint};tls_verify=unsafe_off;token={auth};'\n",
    "        else:\n",
    "            conf = f'http::addr={http_endpoint};'\n",
    "            \n",
    "        with Sender.from_conf(conf) as sender, open(csv_file, mode='r') as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "            events_sent = 0\n",
    "            csv_rows = list(csv_reader)  # Load the CSV data once into memory for looping\n",
    "            sys.stdout.write(f\"Sender {sender_id} started sending events\\n\")\n",
    "            while events_sent < total_events:\n",
    "                row = csv_rows[events_sent % len(csv_rows)]  # Loop over the CSV rows\n",
    "\n",
    "                if TIMESTAMP_FROM_FILE:\n",
    "                    timestamp_dt = datetime.strptime(row['timestamp'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    timestamp_nanos = TimestampNanos(int(timestamp_dt.timestamp() * 1e9))  # Convert to nanoseconds\n",
    "                else:\n",
    "                    timestamp_nanos = TimestampNanos.now()  # Get current time in nanoseconds\n",
    "                \n",
    "                # Ingest the row with the current timestamp\n",
    "                sender.row(\n",
    "                    'trades',\n",
    "                    symbols={'symbol': row['symbol'], 'side': row['side']},\n",
    "                    columns={\n",
    "                        'price': float(row['price']),\n",
    "                        'amount': float(row['amount']),\n",
    "                    },\n",
    "                    at=timestamp_nanos  # Send timestamp in nanoseconds\n",
    "                )\n",
    "\n",
    "                events_sent += 1\n",
    "\n",
    "                # Delay after each event\n",
    "                if delay_ms > 0:\n",
    "                    time.sleep(delay_ms / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "            sys.stdout.write(f\"Sender {sender_id} finished sending {events_sent} events\\n\")\n",
    "\n",
    "    except IngressError as e:\n",
    "        sys.stderr.write(f'Sender {sender_id} got error: {e}\\n')\n",
    "\n",
    "def parallel_send(total_events, num_senders: int):\n",
    "    events_per_sender = total_events // num_senders\n",
    "    remaining_events = total_events % num_senders\n",
    "\n",
    "    sender_events = [events_per_sender] * num_senders\n",
    "    for i in range(remaining_events):  # Distribute the remaining events\n",
    "        sender_events[i] += 1\n",
    "\n",
    "    with Pool(processes=num_senders) as pool:\n",
    "        sender_ids = range(num_senders)\n",
    "        pool.starmap(send, [(sender_id, sender_events[sender_id]) for sender_id in sender_ids])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stdout.write(f'Ingestion started. Connecting to {HTTP_ENDPOINT}\\n')\n",
    "    parallel_send(TOTAL_EVENTS, NUM_SENDERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
